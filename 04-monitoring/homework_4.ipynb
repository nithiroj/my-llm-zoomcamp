{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cff9f79-698d-4e7d-9cc4-6cb5436bb43a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066b8e7f-6f85-4e65-99d3-0da8bf9d0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "github_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv'\n",
    "\n",
    "url = f'{github_url}?raw=1'\n",
    "\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a424aaec-58de-46b6-9ad6-a36c3fc7f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9f79d4-4613-4463-87d3-14f3a7100ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   answer_llm   300 non-null    object\n",
      " 1   answer_orig  300 non-null    object\n",
      " 2   document     300 non-null    object\n",
      " 3   question     300 non-null    object\n",
      " 4   course       300 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a47337c-a3d6-4e2f-9010-859581ce9683",
   "metadata": {},
   "source": [
    "## Q1. Getting the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e51f15e-2d07-4641-a78f-e52579eafafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847de5d8-4b6c-43a2-9e9e-16c2465d8417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_llm = df.iloc[0].answer_llm\n",
    "\n",
    "answer_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf16ac6-fef4-40b0-8342-d6eab479ff9f",
   "metadata": {},
   "source": [
    "### What's the first value of the resulting vector?\n",
    "\n",
    "- [x] -0.42\n",
    "- [ ] -0.22\n",
    "- [ ] -0.02\n",
    "- [ ] 0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28902c60-0036-43ec-aee4-bc06fe331554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42244655"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = embedding_model.encode(answer_llm)\n",
    "\n",
    "v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9abee-ddcc-441e-a43d-8da6f0f298c3",
   "metadata": {},
   "source": [
    "## Q2. Computing the dot product\n",
    "Now for each answer pair, let's create embeddings and compute dot product between them\n",
    "\n",
    "We will put the results (scores) into the evaluations list\n",
    "\n",
    "### What's the 75% percentile of the score?\n",
    "\n",
    "- [ ] 21.67\n",
    "- [x] 31.67\n",
    "- [ ] 41.67\n",
    "- [ ] 51.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9976551e-55ff-4b74-9079-77c89736b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = embedding_model.encode(answer_llm)\n",
    "    v_orig = embedding_model.encode(answer_orig)\n",
    "    \n",
    "    return v_llm.dot(v_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd787892-2b7c-4a5f-b234-9b243e11a1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': 'You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).',\n",
       " 'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       " 'document': '0227b872',\n",
       " 'question': 'Where can I sign up for the course?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = df.to_dict(orient='records')\n",
    "\n",
    "results[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef4decd5-7abf-4704-b22a-4adc50327b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 300/300 [02:20<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "similarity = []\n",
    "\n",
    "for record in tqdm(results):\n",
    "    sim = compute_similarity(record)\n",
    "    similarity.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0504ca3-3248-4b3d-84a2-a73c8030525e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean      27.495996\n",
       "std        6.384742\n",
       "min        4.547924\n",
       "25%       24.307844\n",
       "50%       28.336870\n",
       "75%       31.674309\n",
       "max       39.476013\n",
       "Name: dot, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dot'] = similarity\n",
    "df['dot'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e04ab5-1412-40cc-9db6-56108299667c",
   "metadata": {},
   "source": [
    "## Q3. Computing the cosine\n",
    "From Q2, we can see that the results are not within the `[0, 1]` range. It's because the vectors coming from this model are not normalized.\n",
    "\n",
    "So we need to normalize them.\n",
    "\n",
    "To do it, we\n",
    "\n",
    "Compute the norm of a vector\n",
    "Divide each element by this norm\n",
    "So, for vector `v`, it'll be `v / ||v||`\n",
    "\n",
    "In numpy, this is how you do it:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "norm = np.sqrt((v * v).sum())\n",
    "v_norm = v / norm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9afe65-268f-4c25-92df-aa97165cd722",
   "metadata": {},
   "source": [
    "### What's the 75% cosine in the scores?\n",
    "\n",
    "- [ ] 0.63\n",
    "- [ ] 0.73\n",
    "- [x] 0.83\n",
    "- [ ] 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aea6e0f1-c1a6-4fca-95f2-1cdf6c4ac638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = embedding_model.encode(answer_llm)\n",
    "    v_orig = embedding_model.encode(answer_orig)\n",
    "\n",
    "    # Calculate the dot product\n",
    "    dot_product = np.dot(v_llm, v_orig)\n",
    "    \n",
    "    # Calculate the norm (magnitude)\n",
    "    norm_v_llm = np.linalg.norm(v_llm)\n",
    "    norm_v_orig = np.linalg.norm(v_orig)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = dot_product / (norm_v_llm * norm_v_orig)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "950a8332-6625-40ca-b471-dbad973c9da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 300/300 [02:19<00:00,  2.15it/s]\n"
     ]
    }
   ],
   "source": [
    "cosine_sims = []\n",
    "\n",
    "for record in tqdm(results):\n",
    "    cosine_sim = cosine_similarity(record)\n",
    "    cosine_sims.append(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c48ca54-7e04-4632-b2d6-0ae8e4f73fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean       0.728393\n",
       "std        0.157755\n",
       "min        0.125357\n",
       "25%        0.651274\n",
       "50%        0.763761\n",
       "75%        0.836235\n",
       "max        0.958796\n",
       "Name: cosine, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cosine'] = cosine_sims\n",
    "df['cosine'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12056316-9ea4-44eb-b621-484fb390674b",
   "metadata": {},
   "source": [
    "## Q4. Rouge\n",
    "Now we will explore an alternative metric - the ROUGE score.\n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "We don't need to implement it ourselves, there's a python package for it:\n",
    "\n",
    "```pip install rouge```\n",
    "\n",
    "(The latest version at the moment of writing is `1.0.1`)\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (`doc_id=5170565b`)\n",
    "\n",
    "```\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "```\n",
    "There are three scores: `rouge-1`, `rouge-2` and `rouge-l`, and precision, recall and F1 score for each.\n",
    "\n",
    "- `rouge-1` - the overlap of unigrams,\n",
    "- `rouge-2` - bigrams,\n",
    "- `rouge-l` - the longest common subsequence\n",
    "\n",
    "### What's the F score for rouge-1?\n",
    "\n",
    "- [ ] 0.35\n",
    "- [x] 0.45\n",
    "- [ ] 0.55\n",
    "- [ ] 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "208b2bb6-45b7-4cb5-aef3-944350745750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge_scorer = Rouge()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b0edd63-bf22-4b18-b208-9f46cc0304ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_llm': 'You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).',\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork'},\n",
       " {'answer_llm': 'You can sign up using the link provided in the course GitHub repository: [https://airtable.com/shryxwLd0COOEaqXo](https://airtable.com/shryxwLd0COOEaqXo).',\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['answer_llm', 'answer_orig']].to_dict(orient='records')[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4577a366-9a1c-44f1-805a-29cb812ba1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer_llm     Yes, all sessions are recorded, so if you miss...\n",
       "answer_orig    Everything is recorded, so you won’t miss anyt...\n",
       "document                                                5170565b\n",
       "question                    Are sessions recorded if I miss one?\n",
       "course                                 machine-learning-zoomcamp\n",
       "dot                                                    32.344711\n",
       "cosine                                                  0.777956\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = df.iloc[10]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4893154f-6f1c-4400-9677-7a2746d51d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.45454545454545453,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.45454544954545456},\n",
       " 'rouge-2': {'r': 0.21621621621621623,\n",
       "  'p': 0.21621621621621623,\n",
       "  'f': 0.21621621121621637},\n",
       " 'rouge-l': {'r': 0.3939393939393939,\n",
       "  'p': 0.3939393939393939,\n",
       "  'f': 0.393939388939394}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430145c9-6271-4d17-bdee-a2913f041e40",
   "metadata": {},
   "source": [
    "## Q5. Average rouge score\n",
    "\n",
    "Let's compute the average F-score between `rouge-1`, `rouge-2` and `rouge-l` for the same record from Q4\n",
    "\n",
    "- [x] 0.35\n",
    "- [ ] 0.45\n",
    "- [ ] 0.55\n",
    "- [ ] 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d0337b6-395f-405a-ace5-304d07c6ea82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35490034990035496"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_avg = (scores['rouge-1']['f'] + scores['rouge-2']['f'] + scores['rouge-l']['f']) / 3\n",
    "f_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c29ac2-f468-40af-84e2-b93ceb9729f5",
   "metadata": {},
   "source": [
    "## Q6. Average rouge score for all the data points\n",
    "Now let's compute the score for all the records and create a dataframe from them.\n",
    "\n",
    "### What's the average `rouge_2` across all the records?\n",
    "\n",
    "- [ ] 0.10\n",
    "- [x] 0.20\n",
    "- [ ] 0.30\n",
    "- [ ] 0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "decd9791-2bac-4618-9982-292da13d660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_2_scores = [rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]['rouge-2']['f'] for r in df[['answer_llm', 'answer_orig']].to_dict(orient='records')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7b2301f-62c2-44e9-bc79-0dd32115609a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20696501983423318"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rouge_2_scores =  np.sum(rouge_2_scores) / len(rouge_2_scores)\n",
    "\n",
    "avg_rouge_2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e4521-bfbd-478d-bf62-8187762d1261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
